---
title: "TIL: Linear regression"
date: "03-25-2023"
format:
  html: default
  gfm: default
---

```{r, warning=F, message=F}

library(patchwork)
library(palmerpenguins)
library(faux)
library(GGally)
library(tidyverse)
library(brms)
library(tidybayes)
set.seed(1234)

```

I recently took a workshop on Bayesian regression. But I realized during the courses that I was missing pieces even of the frequentist approach. Below I try to make sure to capture those pieces.


We start with some data, following an example from Gelman et al. (2020). 


```{r}

fake <- tibble(x = seq.int(1, 20, by = 1),
               y = 0.2 + 0.3 * x + 0.5 * rnorm(20))

fake |>
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  theme_minimal()

```
The model is:

$$
\begin{align}
\textit{y}_i & = {\alpha} + {\beta}{x}_i + \epsilon_i\\
\epsilon_i & \sim \operatorname{Normal}(0, \sigma_) 
\end{align}
$$
where the predictor $\text{x}_i$ takes on the values from 1 to 20; the intercept is $\alpha$ = 0.2, the slope is $\beta$ = 0.3, and the errors $\epsilon_i$ are normally distributed with mean 0 and standard deviation $\sigma$ = 0.5. 

Now, let's fit the model.

```{r}

simple <- lm(
  y ~ 1 + x, 
  data = fake
)
  
summary(simple)

```
There is a nice interactive explanation of linear regression here; I guess within the context of machine learning: [Linear Regression](https://mlu-explain.github.io/linear-regression/)

The output shows:

- The formula
- Residuals: the difference between the actual observed _y_ and the _y_ the model predicted, $\hat{y}$. The summary points helps us identify if the distribution is symmetrical. Basically:

```{r}

summary(fake$y - simple$fitted.values)

```

Using the command:

```{r}

residuals(simple)

```

We can plot the residuals. In general, we want those residuals to look gaussian:

```{r}

tibble(r = residuals(simple)) |>
  ggplot(aes(x = r)) +
  geom_histogram(binwidth = 0.5)

```

And sometimes people do a Qâ€“Q plot. Or we can just plot the residuals.

```{r}

ggplot(simple, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_minimal()

```
or 

```{r}

ggplot(simple, aes(x = .resid)) +
  geom_histogram(binwidth = 0.5) +
  scale_x_continuous(limits = c(-1.5, 1.5)) +
  theme_minimal()

```
Gelman et al. (2020) in the section about assumptions of regression analysis mention the following assumptions in order of importance:

- validity,
- representativeness,
- additivity and linearity,
- independence of errors,
- equal variance of errors, and
- normality of errors.

Notice _equal variance of errors_ and _normality of error_ but only toward the end of the list! 

- The coefficients

```{r}

coef(simple)

```

- The standard error of the estimates. 

- The t-statistic from which our p-values came from; the t-statistic is the coefficient divided by the standard error.

- Residual standard error, the square root of a sum of squares term divided by its degrees of freedom (20 data points minus two parameters, intercept and slope). This is the average amount that _y_ will deviate from the regression line. We can get that residual standard error this way:

```{r}

k = length(simple$coefficients)-1

SSE = sum(simple$residuals**2)

n = length(simple$residuals)

sqrt(SSE/(n-(1+k))) 

```

or like this:

```{r}

tibble(r = residuals(simple)) |>
  summarise(m = mean(r),
            s = sd(r))

```
Now plot:

```{r}

fake |>
  ggplot(aes(x = x, y = y )) +
  geom_point() +
  geom_abline(intercept = simple$coefficients[1], slope = simple$coefficients[2], color = "#90be6d") +
  labs(title = "Data and fitted regression line") +
  theme_minimal()

```
We can use predict to give us the model-based predictions of each case in the data. We get one prediction, one fitted value, for each case in the data set.

```{r}

predict(simple)

```
We might want to express the uncertainty around those predictions with confidence intervals.

```{r}

predict(simple,
        interval = "confidence") |>
  # just the top 6
  head()

```
Predict has the interval argument. point estimate, and upper and lower level 95% confidence intervals. We can also ask for the standard errors of each person predictions or the standard error of predicted means:

```{r}

predict(simple,
        se.fit = TRUE) |> 
  data.frame()

```

Or we can use a new data set; that is, define a prediction grid for which we want predictions. The function will then give us predictions based on these new values.

```{r}

new_data <- tibble(x = seq.int(0.5, 20.5, by = 1))

predict(simple,
        interval = "confidence",
        newdata = new_data) |>
  head()
```
Given the original predictor grid, with the bind functions we can bind new columns. Now we can add our predictor values, _x_, along with our predicted values, _y_. In red below, we can see the data in the model, _x_ and _y_, and in green, the new data for the predictions.

```{r}

predict(simple,
        interval = "confidence",
        newdata = new_data) |>
  data.frame() |>
  bind_cols(new_data) |>
  ggplot(aes(x = x)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr),
              alpha = 1/3) +
  geom_line(aes(y = fit)) +
  geom_point(aes(x = x, y = fit), color = "#2bc2c2", alpha = 0.6) +
  geom_point(data = fake,
             aes(x = x, y = y), color= "#ff483b", alpha = 0.6)

```





